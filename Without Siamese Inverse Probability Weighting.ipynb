{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpagare/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: \n",
      "    Found GPU%d %s which is of cuda capability %d.%d.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is %d.%d.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn.format(d, name, major, minor, min_arch // 10, min_arch % 10))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text  opioid_pr_ab  \\\n",
      "6292     \\nname                      unit no   \\n \\nad...         False   \n",
      "92111    \\nname                   unit no   \\n \\nadmis...          True   \n",
      "209235   \\nname                     unit no   \\n \\nadm...         False   \n",
      "225051   \\nname                  unit no   \\n \\nadmiss...          True   \n",
      "143620   \\nname                   unit no   \\n \\nadmis...         False   \n",
      "\n",
      "        determinant_1  determinant_2  determinant_3  determinant_4  \\\n",
      "6292            False           True          False          False   \n",
      "92111           False           True          False          False   \n",
      "209235          False           True          False          False   \n",
      "225051          False           True          False          False   \n",
      "143620          False           True          False          False   \n",
      "\n",
      "        determinant_5  determinant_6  determinant_7  determinant_8  \\\n",
      "6292            False          False          False          False   \n",
      "92111           False          False          False          False   \n",
      "209235          False          False          False          False   \n",
      "225051          False          False          False          False   \n",
      "143620          False          False          False          False   \n",
      "\n",
      "        determinant_9  determinant_10  determinant_11  determinant_12  \\\n",
      "6292             True            True            True           False   \n",
      "92111            True            True           False            True   \n",
      "209235          False            True           False            True   \n",
      "225051           True            True            True            True   \n",
      "143620           True            True            True           False   \n",
      "\n",
      "        determinant_13                                         embeddings  \n",
      "6292              True  [0.11611952, -0.15015103, -0.26891693, 0.21557...  \n",
      "92111            False  [0.10677204, -0.1410134, -0.17166698, 0.336330...  \n",
      "209235            True  [0.12080953, -0.24113658, -0.15327021, 0.23211...  \n",
      "225051            True  [0.11869177, -0.0945933, -0.26454484, 0.145337...  \n",
      "143620            True  [0.022291262, -0.14719284, -0.3362472, 0.28968...  \n",
      "Calculating for determinant_1\n",
      "Calculating for determinant_2\n",
      "Calculating for determinant_3\n",
      "Calculating for determinant_4\n",
      "Calculating for determinant_5\n",
      "Skipping determinant_5: This solver needs samples of at least 2 classes in the data, but the data contains only one class: False\n",
      "Calculating for determinant_6\n",
      "Calculating for determinant_7\n",
      "Calculating for determinant_8\n",
      "Calculating for determinant_9\n",
      "Calculating for determinant_10\n",
      "Calculating for determinant_11\n",
      "Calculating for determinant_12\n",
      "Calculating for determinant_13\n",
      "       Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0    determinant_1                 10000                    3   \n",
      "1    determinant_2                 10000                 9905   \n",
      "2    determinant_3                 10000                   27   \n",
      "3    determinant_4                 10000                   83   \n",
      "4    determinant_6                 10000                    1   \n",
      "5    determinant_7                 10000                   48   \n",
      "6    determinant_8                 10000                 4503   \n",
      "7    determinant_9                 10000                 7735   \n",
      "8   determinant_10                 10000                 9910   \n",
      "9   determinant_11                 10000                 8569   \n",
      "10  determinant_12                 10000                 2372   \n",
      "11  determinant_13                 10000                 9375   \n",
      "\n",
      "    Untreated Sample Size        Y1        Y0       ATE        p-value  \\\n",
      "0                    9997  0.000000  0.033310  0.007094   6.604739e-01   \n",
      "1                      95  0.024432  0.957895  0.029859   0.000000e+00   \n",
      "2                    9973  0.888889  0.030984  0.007259   2.610575e-61   \n",
      "3                    9917  0.349398  0.030654  0.006010   1.108290e-39   \n",
      "4                    9999  0.000000  0.033303  0.008548   4.642054e-01   \n",
      "5                    9952  0.145833  0.032757  0.008842   1.448049e-03   \n",
      "6                    5497  0.006884  0.054939  0.016181   5.984016e-24   \n",
      "7                    2265  0.042405  0.002208 -0.017200   2.190242e-06   \n",
      "8                      90  0.032392  0.133333  0.010137   6.849464e-08   \n",
      "9                    1431  0.019489  0.116003  0.071789   1.681870e-51   \n",
      "10                   7628  0.130270  0.003146 -0.017399  5.944332e-103   \n",
      "11                    625  0.013867  0.324800 -0.015814  1.211194e-250   \n",
      "\n",
      "    95% CI Lower  95% CI Upper  \n",
      "0       0.007094      0.007094  \n",
      "1       0.029859      0.029859  \n",
      "2       0.007259      0.007259  \n",
      "3       0.006010      0.006010  \n",
      "4       0.008548      0.008548  \n",
      "5       0.008842      0.008842  \n",
      "6       0.016181      0.016181  \n",
      "7      -0.017200     -0.017200  \n",
      "8       0.010137      0.010137  \n",
      "9       0.071789      0.071789  \n",
      "10     -0.017399     -0.017399  \n",
      "11     -0.015814     -0.015814  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "\n",
    "# Step 1: Generating Embeddings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Disable tokenizers parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load the tokenizer and model for BioClinicalBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(device)\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'cleaned_predictions_2ndjune.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the determinant_pr_ab column\n",
    "df = df.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Take a sample of 10000 rows\n",
    "df = df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Function to generate embeddings\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# Generate embeddings for the 'text' column\n",
    "df['embeddings'] = df['text'].apply(lambda x: get_embeddings(x).squeeze())\n",
    "\n",
    "# Extract embeddings to a separate variable and save as a numpy file\n",
    "embeddings = np.vstack(df['embeddings'].values)\n",
    "np.save('embeddings_sampled.npy', embeddings)\n",
    "\n",
    "# Optionally, save the dataframe with embeddings to a new CSV file\n",
    "df.to_csv('predictions_with_embeddings_sampled.csv', index=False)\n",
    "\n",
    "# Display the head of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Causal Inference with Propensity Score Matching\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataframe with embeddings\n",
    "df = pd.read_csv('predictions_with_embeddings_sampled.csv')\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load('embeddings_sampled.npy')\n",
    "\n",
    "# Flatten the embeddings to ensure they are in the correct format\n",
    "flattened_embeddings = [embedding.flatten() for embedding in embeddings]\n",
    "\n",
    "# Add flattened embeddings to the dataframe\n",
    "df = df.drop(columns=['embeddings'])  # Remove the old embeddings column if it exists\n",
    "flattened_embeddings_df = pd.DataFrame(flattened_embeddings, index=df.index)\n",
    "df = pd.concat([df.reset_index(drop=True), flattened_embeddings_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "embedding_columns = flattened_embeddings_df.columns.tolist()\n",
    "confounders = embedding_columns\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df[outcome] = pd.to_numeric(df[outcome], errors='coerce')\n",
    "\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    \n",
    "    # Ensure the target variable is binary for logistic regression\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(df, X_data, T, y):\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, df[T]).predict_proba(X_data)[:, 1]\n",
    "    weight = (df[T] - ps) / (ps * (1 - ps))  # define the weights\n",
    "    return np.mean(weight * df[y])  # compute the ATE\n",
    "\n",
    "# Initialize lists to store the results\n",
    "determinants = []\n",
    "original_sample_sizes = []\n",
    "treated_sample_sizes = []\n",
    "untreated_sample_sizes = []\n",
    "Y1_values = []\n",
    "Y0_values = []\n",
    "ATE_values = []\n",
    "p_values = []\n",
    "ci_lowers = []\n",
    "ci_uppers = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Loop through each determinant\n",
    "for determinant in df.columns[2:15]:  # Skipping 'text' and 'opioid_pr_ab', and only taking the determinant columns\n",
    "    print(f\"Calculating for {determinant}\")\n",
    "    \n",
    "    # Ensure the determinant column is numeric\n",
    "    df[determinant] = pd.to_numeric(df[determinant], errors='coerce')\n",
    "    \n",
    "    # Calculate propensity scores\n",
    "    try:\n",
    "        df['propensity_score'] = calculate_propensity_scores(df, determinant, confounders)\n",
    "        \n",
    "        # Perform matching and estimate ATE using bootstrap sampling\n",
    "        X_data = df[confounders].values\n",
    "        y_data = df[outcome]\n",
    "        \n",
    "        # Original sample sizes\n",
    "        original_sample_size = len(df)\n",
    "        treated_sample_size = df[df[determinant] == 1].shape[0]\n",
    "        untreated_sample_size = df[df[determinant] == 0].shape[0]\n",
    "        \n",
    "        # Run 1000 bootstrap samples\n",
    "        bootstrap_sample = 1000\n",
    "        ates = Parallel(n_jobs=-1)(delayed(run_ps)(df.sample(frac=1, replace=True, random_state=42).reset_index(drop=True), X_data, determinant, y_data)\n",
    "                                   for _ in range(bootstrap_sample))\n",
    "        ates = np.array(ates)\n",
    "        ci_lower = np.percentile(ates, 2.5)\n",
    "        ci_upper = np.percentile(ates, 97.5)\n",
    "        ATE = np.mean(ates)\n",
    "        \n",
    "        # Fit OLS model for p-value\n",
    "        model = sm.OLS(df[outcome], sm.add_constant(df[[determinant, 'propensity_score']].astype(float)))\n",
    "        result = model.fit()\n",
    "        p_value = result.pvalues[determinant]\n",
    "        \n",
    "        # Y1 and Y0 values\n",
    "        Y1 = df.loc[df[determinant] == 1, outcome].mean()\n",
    "        Y0 = df.loc[df[determinant] == 0, outcome].mean()\n",
    "        \n",
    "        determinants.append(determinant)\n",
    "        original_sample_sizes.append(original_sample_size)\n",
    "        treated_sample_sizes.append(treated_sample_size)\n",
    "        untreated_sample_sizes.append(untreated_sample_size)\n",
    "        Y1_values.append(Y1)\n",
    "        Y0_values.append(Y0)\n",
    "        ATE_values.append(ATE)\n",
    "        p_values.append(p_value)\n",
    "        ci_lowers.append(ci_lower)\n",
    "        ci_uppers.append(ci_upper)\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {determinant}: {e}\")\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Determinant': determinants,\n",
    "    'Original Sample Size': original_sample_sizes,\n",
    "    'Treated Sample Size': treated_sample_sizes,\n",
    "    'Untreated Sample Size': untreated_sample_sizes,\n",
    "    'Y1': Y1_values,\n",
    "    'Y0': Y0_values,\n",
    "    'ATE': ATE_values,\n",
    "    'p-value': p_values,\n",
    "    '95% CI Lower': ci_lowers,\n",
    "    '95% CI Upper': ci_uppers\n",
    "})\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "results_df.to_csv('causal_inference_results_3k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for determinant 5:\n",
      "Original Sample Size: 10000\n",
      "Treated Sample Size: 4966\n",
      "Untreated Sample Size: 5034\n",
      "Y1: 0.03201772049939589\n",
      "Y0: 0.037544696066746125\n",
      "ATE: 0.00016230458592381326\n",
      "p-value: 0.39561241867817076\n",
      "95% CI Lower: -0.06638903469416536\n",
      "95% CI Upper: 0.058431122968938376\n",
      "CATE Results:\n",
      "Treated: 0.03201772049939589\n",
      "Untreated: 0.037544696066746125\n",
      "CATE: -0.005526975567350233\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the dataframe with embeddings\n",
    "df = pd.read_csv('predictions_with_embeddings_sampled.csv')\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load('embeddings_sampled.npy')\n",
    "\n",
    "# Flatten the embeddings to ensure they are in the correct format\n",
    "flattened_embeddings = [embedding.flatten() for embedding in embeddings]\n",
    "\n",
    "# Add flattened embeddings to the dataframe\n",
    "df = df.drop(columns=['embeddings'])  # Remove the old embeddings column if it exists\n",
    "flattened_embeddings_df = pd.DataFrame(flattened_embeddings, index=df.index)\n",
    "df = pd.concat([df.reset_index(drop=True), flattened_embeddings_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Take a sample of 10,000 rows to maintain consistency\n",
    "df = df.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "embedding_columns = flattened_embeddings_df.columns.tolist()\n",
    "confounders = embedding_columns\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df[outcome] = pd.to_numeric(df[outcome], errors='coerce')\n",
    "\n",
    "# Handling class imbalance for determinant 5 by manual oversampling\n",
    "determinant = 'determinant_5'\n",
    "X = df[confounders].values\n",
    "y = df[determinant]\n",
    "\n",
    "# Manually oversample the minority class\n",
    "if len(np.unique(y)) < 2:\n",
    "    print(f\"Skipping manual oversampling for {determinant} due to insufficient samples in one class.\")\n",
    "else:\n",
    "    class_counts = np.bincount(y)\n",
    "    majority_class = np.argmax(class_counts)\n",
    "    minority_class = 1 - majority_class\n",
    "    minority_class_count = class_counts[minority_class]\n",
    "    majority_class_count = class_counts[majority_class]\n",
    "\n",
    "    # Duplicate the minority class samples until they match the majority class\n",
    "    minority_class_indices = np.where(y == minority_class)[0]\n",
    "    oversample_indices = np.random.choice(minority_class_indices, majority_class_count, replace=True)\n",
    "    \n",
    "    X_oversampled = np.vstack((X, X[oversample_indices]))\n",
    "    y_oversampled = np.hstack((y, y[oversample_indices]))\n",
    "\n",
    "    # Update the dataframe with the oversampled data\n",
    "    df_resampled = pd.DataFrame(X_oversampled, columns=confounders)\n",
    "    df_resampled[determinant] = y_oversampled\n",
    "    df_resampled[outcome] = df[outcome].sample(n=len(df_resampled), replace=True, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent sample size of 10,000 after oversampling\n",
    "    df_resampled = df_resampled.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Function to calculate propensity scores\n",
    "    def calculate_propensity_scores(df, treatment, confounders):\n",
    "        X = df[confounders].values\n",
    "        y = df[treatment]\n",
    "        \n",
    "        # Ensure the target variable is binary for logistic regression\n",
    "        if len(np.unique(y)) == 2:\n",
    "            model = LogisticRegression(max_iter=5000)\n",
    "            model.fit(X, y)\n",
    "            propensity_scores = model.predict_proba(X)[:, 1]\n",
    "            return propensity_scores\n",
    "        else:\n",
    "            raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "    # Function to run propensity score matching and calculate ATE\n",
    "    def run_ps(df, X_data, T, y):\n",
    "        ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, df[T]).predict_proba(X_data)[:, 1]\n",
    "        weight = (df[T] - ps) / (ps * (1 - ps))  # define the weights\n",
    "        return np.mean(weight * df[y])  # compute the ATE\n",
    "\n",
    "    # Calculate propensity scores for determinant 5\n",
    "    df_resampled['propensity_score'] = calculate_propensity_scores(df_resampled, determinant, confounders)\n",
    "\n",
    "    # Perform matching and estimate ATE using bootstrap sampling\n",
    "    X_data = df_resampled[confounders].values\n",
    "    y_data = df_resampled[outcome]\n",
    "\n",
    "    # Original sample sizes\n",
    "    original_sample_size = len(df_resampled)\n",
    "    treated_sample_size = df_resampled[df_resampled[determinant] == 1].shape[0]\n",
    "    untreated_sample_size = df_resampled[df_resampled[determinant] == 0].shape[0]\n",
    "\n",
    "    # Run 1000 bootstrap samples\n",
    "    bootstrap_sample = 1000\n",
    "    ates = Parallel(n_jobs=-1)(delayed(run_ps)(df_resampled.sample(frac=1, replace=True).reset_index(drop=True), X_data, determinant, y_data)\n",
    "                               for _ in range(bootstrap_sample))\n",
    "    ates = np.array(ates)\n",
    "    ci_lower = np.percentile(ates, 2.5)\n",
    "    ci_upper = np.percentile(ates, 97.5)\n",
    "    ATE = np.mean(ates)\n",
    "\n",
    "    # Fit OLS model for p-value\n",
    "    model = sm.OLS(df_resampled[outcome], sm.add_constant(df_resampled[[determinant, 'propensity_score']].astype(float)))\n",
    "    result = model.fit()\n",
    "    p_value = result.pvalues[determinant]\n",
    "\n",
    "    # Y1 and Y0 values\n",
    "    Y1 = df_resampled.loc[df_resampled[determinant] == 1, outcome].mean()\n",
    "    Y0 = df_resampled.loc[df_resampled[determinant] == 0, outcome].mean()\n",
    "\n",
    "    # Print the results for determinant 5\n",
    "    print(f\"Results for determinant 5:\")\n",
    "    print(f\"Original Sample Size: {original_sample_size}\")\n",
    "    print(f\"Treated Sample Size: {treated_sample_size}\")\n",
    "    print(f\"Untreated Sample Size: {untreated_sample_size}\")\n",
    "    print(f\"Y1: {Y1}\")\n",
    "    print(f\"Y0: {Y0}\")\n",
    "    print(f\"ATE: {ATE}\")\n",
    "    print(f\"p-value: {p_value}\")\n",
    "    print(f\"95% CI Lower: {ci_lower}\")\n",
    "    print(f\"95% CI Upper: {ci_upper}\")\n",
    "    \n",
    "    # Calculate Conditional Average Treatment Effect (CATE)\n",
    "    def calculate_cate(df, treatment, outcome):\n",
    "        cate_result = {}\n",
    "        treated = df[df[treatment] == 1]\n",
    "        untreated = df[df[treatment] == 0]\n",
    "        cate_result['Treated'] = treated[outcome].mean()\n",
    "        cate_result['Untreated'] = untreated[outcome].mean()\n",
    "        cate_result['CATE'] = treated[outcome].mean() - untreated[outcome].mean()\n",
    "        return cate_result\n",
    "\n",
    "    # Calculate CATE for determinant 5\n",
    "    cate_result = calculate_cate(df_resampled, determinant, outcome)\n",
    "\n",
    "    # Print CATE results\n",
    "    print(\"CATE Results:\")\n",
    "    for key, value in cate_result.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATE Results:\n",
      "Treated (Y1): 0.03201772049939589\n",
      "Untreated (Y0): 0.037544696066746125\n",
      "CATE: -0.005526975567350233\n"
     ]
    }
   ],
   "source": [
    "# Given values\n",
    "Y1 = 0.03201772049939589\n",
    "Y0 = 0.037544696066746125\n",
    "\n",
    "# Calculate CATE\n",
    "CATE = Y1 - Y0\n",
    "\n",
    "# Print CATE results\n",
    "print(\"CATE Results:\")\n",
    "print(f\"Treated (Y1): {Y1}\")\n",
    "print(f\"Untreated (Y0): {Y0}\")\n",
    "print(f\"CATE: {CATE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
