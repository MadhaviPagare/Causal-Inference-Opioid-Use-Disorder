{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLwTz1SyqDG6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Define the model class\n",
    "class HierarchicalClassifier(nn.Module):\n",
    "    def __init__(self, num_labels_h3):\n",
    "        super(HierarchicalClassifier, self).__init__()\n",
    "        self.longformer = AutoModel.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc_h1 = nn.Linear(self.longformer.config.hidden_size, 1)\n",
    "        self.fc_h2 = nn.Linear(self.longformer.config.hidden_size, 1)\n",
    "        self.fc_h3 = nn.Linear(self.longformer.config.hidden_size, num_labels_h3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.longformer(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits_h1 = self.fc_h1(self.dropout(pooled_output))\n",
    "        logits_h2 = self.fc_h2(self.dropout(pooled_output))\n",
    "        logits_h3 = self.fc_h3(self.dropout(pooled_output))\n",
    "        return logits_h1, logits_h2, logits_h3\n",
    "\n",
    "# Load the saved model\n",
    "num_labels_h3 = 13  # Number of determinants\n",
    "model = HierarchicalClassifier(num_labels_h3)\n",
    "model.load_state_dict(torch.load('model_Clinical-Longformer.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_texts(texts, tokenizer, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text, None, add_special_tokens=True, max_length=max_length,\n",
    "            padding='max_length', return_token_type_ids=True, truncation=True\n",
    "        )\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        attention_masks.append(inputs['attention_mask'])\n",
    "    return torch.tensor(input_ids, dtype=torch.long), torch.tensor(attention_masks, dtype=torch.long)\n",
    "\n",
    "# Function to make predictions for a batch of texts\n",
    "def predict_batch(texts, model, tokenizer, device):\n",
    "    model.to(device)\n",
    "    input_ids, attention_masks = preprocess_texts(texts, tokenizer)\n",
    "    input_ids, attention_masks = input_ids.to(device), attention_masks.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_h1, logits_h2, logits_h3 = model(input_ids, attention_masks)\n",
    "        pred_h1 = torch.sigmoid(logits_h1).cpu().numpy() > 0.5\n",
    "        pred_h2 = torch.sigmoid(logits_h2).cpu().numpy() > 0.5\n",
    "        pred_h3 = torch.sigmoid(logits_h3).cpu().numpy() > 0.5\n",
    "\n",
    "    return pred_h1, pred_h2, pred_h3\n",
    "\n",
    "# Process the data in chunks and mini-batches\n",
    "chunk_size = 10000  # Define the chunk size\n",
    "batch_size = 32     # Define the batch size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize an empty DataFrame to store the predictions\n",
    "predictions_df = pd.DataFrame(columns=['text', 'opioid_pr_ab', 'determinant_pr_ab', *['determinant_' + str(i) for i in range(1, 14)]])\n",
    "\n",
    "# Load the discharge.csv file in chunks\n",
    "for chunk in pd.read_csv('discharge.csv', chunksize=chunk_size):\n",
    "    chunk_predictions = []\n",
    "    texts = chunk['text'].tolist()\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        pred_h1, pred_h2, pred_h3 = predict_batch(batch_texts, model, tokenizer, device)\n",
    "        for text, h1, h2, h3 in zip(batch_texts, pred_h1, pred_h2, pred_h3):\n",
    "            chunk_predictions.append([text, h1[0], h2[0], *h3])\n",
    "\n",
    "    # Convert chunk predictions to a DataFrame and append to the main DataFrame\n",
    "    chunk_predictions_df = pd.DataFrame(chunk_predictions, columns=['text', 'opioid_pr_ab', 'determinant_pr_ab', *['determinant_' + str(i) for i in range(1, 14)]])\n",
    "    predictions_df = pd.concat([predictions_df, chunk_predictions_df], ignore_index=True)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictions_2ndjune.csv', index=False)\n",
    "\n",
    "# Load and print the first 5 rows of predictions.csv\n",
    "#predictions_df = pd.read_csv('predictions.csv')\n",
    "#print(predictions_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOnPxW9blwuPnJp7q/56HDJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
