{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant 4 below used in final table in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_4:\n",
      "determinant_4\n",
      "False    328978\n",
      "True       2815\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_4:\n",
      "determinant_4\n",
      "0    328978\n",
      "1    328978\n",
      "Name: count, dtype: int64\n",
      "20561/20561 [==============================] - 43s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_4:\n",
      "determinant_4\n",
      "1    5034\n",
      "0    4966\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (657925, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_4'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_4'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_4\n",
      "Class distribution for determinant_4:\n",
      "determinant_4\n",
      "1    5034\n",
      "0    4966\n",
      "Name: count, dtype: int64\n",
      "     Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_4                 10000                 5034   \n",
      "\n",
      "   Untreated Sample Size       Y1        Y0       ATE      CATE ..  \\\n",
      "0                   4966  0.40233  0.029016  0.187476  0.373071   ..      \n",
      "\n",
      "   95% CI Lower  95% CI Upper  \n",
      "0      0.179297      0.196402  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)]  \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_4']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_4', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()\n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]   # 768-dim BioClinicalBERT embeddings\n",
    "\n",
    "\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 4)\n",
    "treatment_columns = ['determinant_4']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant 7 used in final table in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_7:\n",
      "determinant_7\n",
      "False    330170\n",
      "True       1623\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_7:\n",
      "determinant_7\n",
      "0    330170\n",
      "1    330170\n",
      "Name: count, dtype: int64\n",
      "20635/20635 [==============================] - 43s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_7:\n",
      "determinant_7\n",
      "0    5034\n",
      "1    4966\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (660309, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_7'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_7'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_7\n",
      "Class distribution for determinant_7:\n",
      "determinant_7\n",
      "0    5034\n",
      "1    4966\n",
      "Name: count, dtype: int64\n",
      "     Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_7                 10000                 4966   \n",
      "\n",
      "   Untreated Sample Size        Y1        Y0       ATE      CATE  \\\n",
      "0                   5034  0.082111  0.034479  0.023299  0.047591   \n",
      "\n",
      "        p-value  95% CI Lower  95% CI Upper  \n",
      "0  3.886153e-58      0.018597      0.027902  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)]  \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_7']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_7', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]   # 768-dim BioClinicalBERT embeddings\n",
    "\n",
    "\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 7)\n",
    "treatment_columns = ['determinant_7']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_7.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant 6 did not  use this below result. we used determinant 6 value from previous siamese table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_6:\n",
      "determinant_6\n",
      "False    331760\n",
      "True         33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_6:\n",
      "determinant_6\n",
      "0    331760\n",
      "1    331760\n",
      "Name: count, dtype: int64\n",
      "20735/20735 [==============================] - 47s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_6:\n",
      "determinant_6\n",
      "1    5015\n",
      "0    4985\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (663489, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_6'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_6'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_6\n",
      "Class distribution for determinant_6:\n",
      "determinant_6\n",
      "1    5015\n",
      "0    4985\n",
      "Name: count, dtype: int64\n",
      "     Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_6                 10000                 5015   \n",
      "\n",
      "   Untreated Sample Size   Y1        Y0       ATE      CATE       p-value  \\\n",
      "0                   4985  0.0  0.031945 -0.015945 -0.031896  1.634616e-11   \n",
      "\n",
      "   95% CI Lower  95% CI Upper  \n",
      "0       -0.0184       -0.0134  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)]  \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_6']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_6', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]   # 768-dim BioClinicalBERT embeddings\n",
    "\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 6)\n",
    "treatment_columns = ['determinant_6']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_6.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant 5 we did not use this below result.we used old siamese results for final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_5:\n",
      "determinant_5\n",
      "False    331682\n",
      "True        111\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_5:\n",
      "determinant_5\n",
      "0    331682\n",
      "1    331682\n",
      "Name: count, dtype: int64\n",
      "20730/20730 [==============================] - 47s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_5:\n",
      "determinant_5\n",
      "0    5016\n",
      "1    4984\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (663333, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_5'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_5'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_5\n",
      "Class distribution for determinant_5:\n",
      "determinant_5\n",
      "0    5016\n",
      "1    4984\n",
      "Name: count, dtype: int64\n",
      "     Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_5                 10000                 4984   \n",
      "\n",
      "   Untreated Sample Size        Y1        Y0       ATE      CATE  \\\n",
      "0                   5016  0.123437  0.031036  0.045997  0.092293   \n",
      "\n",
      "        p-value  95% CI Lower  95% CI Upper  \n",
      "0  1.800373e-52      0.040655      0.051002  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)]  \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_5']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_5', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]   \n",
    "\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 5)\n",
    "treatment_columns = ['determinant_5']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant 10 we used this below result for our final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_10:\n",
      "determinant_10\n",
      "True     328834\n",
      "False      2959\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_10:\n",
      "determinant_10\n",
      "1    328834\n",
      "0    328834\n",
      "Name: count, dtype: int64\n",
      "20552/20552 [==============================] - 48s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_10:\n",
      "determinant_10\n",
      "1    5045\n",
      "0    4955\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (657637, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_10'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_10'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_10\n",
      "Class distribution for determinant_10:\n",
      "determinant_10\n",
      "1    5045\n",
      "0    4955\n",
      "Name: count, dtype: int64\n",
      "      Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_10                 10000                 5045   \n",
      "\n",
      "   Untreated Sample Size       Y1        Y0       ATE      CATE  \\\n",
      "0                   4955  0.03162  0.106544 -0.036953 -0.074841   \n",
      "\n",
      "         p-value  95% CI Lower  95% CI Upper  \n",
      "0  8.755014e-229       -0.0419       -0.0319  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)]  \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_10']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_10', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist() \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]   \n",
    "\n",
    "\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 10)\n",
    "treatment_columns = ['determinant_10']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_10.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant 12 we used below result for determinant 12 in our paper final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_12:\n",
      "determinant_12\n",
      "False    252156\n",
      "True      79637\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_12:\n",
      "determinant_12\n",
      "1    252156\n",
      "0    252156\n",
      "Name: count, dtype: int64\n",
      "15759/15759 [==============================] - 34s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_12:\n",
      "determinant_12\n",
      "1    5095\n",
      "0    4905\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (504263, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_12'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_12'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_12\n",
      "Class distribution for determinant_12:\n",
      "determinant_12\n",
      "1    5095\n",
      "0    4905\n",
      "Name: count, dtype: int64\n",
      "      Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_12                 10000                 5095   \n",
      "\n",
      "   Untreated Sample Size        Y1        Y0       ATE     CATE ..  \\\n",
      "0                   4905  0.124582  0.003487  0.060525  0.12097   ..     \n",
      "\n",
      "   95% CI Lower  95% CI Upper  \n",
      "0      0.044121        0.0665  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)]  \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_12']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_12', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 12)\n",
    "treatment_columns = ['determinant_12']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_12.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant 8 we used this below results for final paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_8:\n",
      "determinant_8\n",
      "False    183286\n",
      "True     148507\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_8:\n",
      "determinant_8\n",
      "0    183286\n",
      "1    183286\n",
      "Name: count, dtype: int64\n",
      "11455/11455 [==============================] - 27s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_8:\n",
      "determinant_8\n",
      "0    5033\n",
      "1    4967\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (366541, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_8'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_8'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_8\n",
      "Class distribution for determinant_8:\n",
      "determinant_8\n",
      "0    5033\n",
      "1    4967\n",
      "Name: count, dtype: int64\n",
      "     Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_8                 10000                 4967   \n",
      "\n",
      "   Untreated Sample Size        Y1      Y0       ATE      CATE        p-value  \\\n",
      "0                   5033  0.005456  0.0609 -0.027919 -0.055363  2.568839e-244   \n",
      "\n",
      "   95% CI Lower  95% CI Upper  \n",
      "0       -0.0314     -0.024297  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)] \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_8']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_8', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]\n",
    "\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 8)\n",
    "treatment_columns = ['determinant_8']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_8.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant 9 we used this below resulst for final paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_9:\n",
      "determinant_9\n",
      "True     257381\n",
      "False     74412\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_9:\n",
      "determinant_9\n",
      "1    257381\n",
      "0    257381\n",
      "Name: count, dtype: int64\n",
      "16086/16086 [==============================] - 37s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_9:\n",
      "determinant_9\n",
      "0    5036\n",
      "1    4964\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (514729, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_9'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_9'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_9\n",
      "Class distribution for determinant_9:\n",
      "determinant_9\n",
      "0    5036\n",
      "1    4964\n",
      "Name: count, dtype: int64\n",
      "     Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_9                 10000                 4964   \n",
      "\n",
      "   Untreated Sample Size        Y1        Y0       ATE      CATE  \\\n",
      "0                   5036  0.045192  0.001197  0.021832  0.043933   \n",
      "\n",
      "        p-value  95% CI Lower  95% CI Upper  \n",
      "0  2.035335e-72      0.019097      0.024802  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)] \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_9']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_9', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 9)\n",
    "treatment_columns = ['determinant_9']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_9.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant1 not used this below results.used results from old table only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_1:\n",
      "determinant_1\n",
      "False    331531\n",
      "True        262\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_1:\n",
      "determinant_1\n",
      "0    331531\n",
      "1    331531\n",
      "Name: count, dtype: int64\n",
      "20720/20720 [==============================] - 45s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_1:\n",
      "determinant_1\n",
      "0    5015\n",
      "1    4985\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (663031, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_1'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_1'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_1\n",
      "Class distribution for determinant_1:\n",
      "determinant_1\n",
      "0    5015\n",
      "1    4985\n",
      "Name: count, dtype: int64\n",
      "     Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_1                 10000                 4985   \n",
      "\n",
      "   Untreated Sample Size        Y1        Y0       ATE      CATE   p-value  \\\n",
      "0                   5015  0.002614  0.034359 -0.015866 -0.031689  0.000003   \n",
      "\n",
      "   95% CI Lower  95% CI Upper  \n",
      "0       -0.0185       -0.0132  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)]  \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_1']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_1', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 1)\n",
    "treatment_columns = ['determinant_1']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant11 not used this below results.used new results from windows mach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_11:\n",
      "determinant_11\n",
      "True     285171\n",
      "False     46622\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_11:\n",
      "determinant_11\n",
      "1    285171\n",
      "0    285171\n",
      "Name: count, dtype: int64\n",
      "17823/17823 [==============================] - 40s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_11:\n",
      "determinant_11\n",
      "1    5017\n",
      "0    4983\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (570308, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_11'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_11'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_11\n",
      "Class distribution for determinant_11:\n",
      "determinant_11\n",
      "1    5017\n",
      "0    4983\n",
      "Name: count, dtype: int64\n",
      "      Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_11                 10000                 5017   \n",
      "\n",
      "   Untreated Sample Size        Y1        Y0       ATE     CATE  \\\n",
      "0                   4983  0.025614  0.112793 -0.043365 -0.08707   \n",
      "\n",
      "         p-value  95% CI Lower  95% CI Upper  \n",
      "0  5.969246e-104       -0.0485     -0.038397  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)] \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_11']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_11', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 11)\n",
    "treatment_columns = ['determinant_11']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_11.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant2 not used these below results.we used new results from windows mach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_2:\n",
      "determinant_2\n",
      "True     328694\n",
      "False      3099\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_2:\n",
      "determinant_2\n",
      "1    328694\n",
      "0    328694\n",
      "Name: count, dtype: int64\n",
      "20543/20543 [==============================] - 49s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_2:\n",
      "determinant_2\n",
      "0    5020\n",
      "1    4980\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (657357, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_2'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_2'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_2\n",
      "Class distribution for determinant_2:\n",
      "determinant_2\n",
      "0    5020\n",
      "1    4980\n",
      "Name: count, dtype: int64\n",
      "     Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_2                 10000                 4980   \n",
      "\n",
      "   Untreated Sample Size        Y1        Y0       ATE      CATE  p-value  \\\n",
      "0                   5020  0.023281  0.978639 -0.483886 -0.954801      0.01   \n",
      "\n",
      "   95% CI Lower  95% CI Upper  \n",
      "0     -0.493099     -0.470197  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)]  \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_2']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_2', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 2)\n",
    "treatment_columns = ['determinant_2']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant3 not used these below results.we used new results from windows mach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_3:\n",
      "determinant_3\n",
      "False    330722\n",
      "True       1071\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_3:\n",
      "determinant_3\n",
      "0    330722\n",
      "1    330722\n",
      "Name: count, dtype: int64\n",
      "20670/20670 [==============================] - 48s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_3:\n",
      "determinant_3\n",
      "1    5100\n",
      "0    4900\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (661414, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_3'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_3'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_3\n",
      "Class distribution for determinant_3:\n",
      "determinant_3\n",
      "1    5100\n",
      "0    4900\n",
      "Name: count, dtype: int64\n",
      "     Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_3                 10000                 5100   \n",
      "\n",
      "   Untreated Sample Size        Y1        Y0       ATE      CATE ..  \\\n",
      "0                   4900  0.859942  0.030354  0.413967  0.829224  ..       \n",
      "\n",
      "   95% CI Lower  95% CI Upper  \n",
      "0      0.322766        0.4333  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)]  \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_3']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_3', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 3)\n",
    "treatment_columns = ['determinant_3']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts before balancing:\n",
      "determinant_13:\n",
      "determinant_13\n",
      "True     311212\n",
      "False     20581\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts after balancing for determinant_13:\n",
      "determinant_13\n",
      "1    311212\n",
      "0    311212\n",
      "Name: count, dtype: int64\n",
      "19449/19449 [==============================] - 44s 2ms/step\n",
      "Class counts in the final subsampled dataframe:\n",
      "determinant_13:\n",
      "determinant_13\n",
      "0    5016\n",
      "1    4984\n",
      "Name: count, dtype: int64\n",
      "Subgroup shape: (622367, 770)\n",
      "Subsampled dataframe shape: (10000, 770)\n",
      "Columns in the subgroup:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_13'],\n",
      "      dtype='object', length=770)\n",
      "Columns in the subsampled dataframe:\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '760', '761', '762', '763', '764', '765', '766', '767', 'opioid_pr_ab',\n",
      "       'determinant_13'],\n",
      "      dtype='object', length=770)\n",
      "Calculating for determinant_13\n",
      "Class distribution for determinant_13:\n",
      "determinant_13\n",
      "0    5016\n",
      "1    4984\n",
      "Name: count, dtype: int64\n",
      "      Determinant  Original Sample Size  Treated Sample Size  \\\n",
      "0  determinant_13                 10000                 4984   \n",
      "\n",
      "   Untreated Sample Size        Y1        Y0       ATE      CATE  ..  \\\n",
      "0                   5016  0.013752  0.342436 -0.164915 -0.328462         \n",
      "\n",
      "   95% CI Lower  95% CI Upper  \n",
      "0       -0.1725        -0.157  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Step 1: Data Loading and SMOTE Balancing\n",
    "# Load your data in chunks\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('predictions_with_embeddings_sampled.csv', chunksize=chunk_size)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# Ensure that the outcome column is numeric\n",
    "df['opioid_pr_ab'] = pd.to_numeric(df['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Features including embedding columns\n",
    "embedding_columns = [str(i) for i in range(768)]  \n",
    "features = embedding_columns\n",
    "\n",
    "# Initialize SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "cols_to_balance = ['determinant_13']\n",
    "\n",
    "# Print class counts before balancing\n",
    "print(\"Class counts before balancing:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "\n",
    "# Apply SMOTE to balance classes\n",
    "balanced_data_list = []\n",
    "for col in cols_to_balance:\n",
    "    if col in df.columns:\n",
    "        df_features = df[features + ['opioid_pr_ab']]  # Include the 'opioid_pr_ab' column for SMOTE\n",
    "        df_target = df[col].astype(int)\n",
    "        df_features_balanced, df_target_balanced = smote.fit_resample(df_features, df_target)\n",
    "        \n",
    "        # Combine the balanced features and target back into a dataframe\n",
    "        df_balanced = pd.concat([df_features_balanced, pd.Series(df_target_balanced, name=col)], axis=1)\n",
    "        balanced_data_list.append(df_balanced)\n",
    "\n",
    "        # Print class counts after balancing for each determinant\n",
    "        print(f\"\\nClass counts after balancing for {col}:\")\n",
    "        print(pd.Series(df_target_balanced).value_counts())\n",
    "\n",
    "        # Clear memory\n",
    "        del df_features, df_target, df_features_balanced, df_target_balanced, df_balanced\n",
    "        gc.collect()\n",
    "\n",
    "# Combine the balanced dataframes\n",
    "df_balanced_final = pd.concat(balanced_data_list, axis=0).drop_duplicates().reset_index(drop=True)\n",
    "del balanced_data_list  # Free memory\n",
    "\n",
    "# Ensure no duplicate columns after merging\n",
    "df_balanced_final = df_balanced_final.loc[:, ~df_balanced_final.columns.duplicated()]\n",
    "\n",
    "# Step 2: Create Siamese Neural Network Model\n",
    "def create_siamese_nn(input_dim, hidden_dim, dropout_prob):\n",
    "    x = Input(shape=(input_dim,), name='x')\n",
    "    shared = Dense(hidden_dim, activation='relu')(x)\n",
    "    shared = Dropout(dropout_prob)(shared)\n",
    "    t1 = Input(shape=(1,), name='t1')\n",
    "    t1_shared = Dense(hidden_dim, activation='relu')(t1)\n",
    "    t1_shared = Dropout(dropout_prob)(t1_shared)\n",
    "    t1_output = Dense(1, activation='linear')(Concatenate()([shared, t1_shared]))\n",
    "    t0 = Input(shape=(1,), name='t0')\n",
    "    t0_shared = Dense(hidden_dim, activation='relu')(t0)\n",
    "    t0_shared = Dropout(dropout_prob)(t0_shared)\n",
    "    t0_output = Dense(1, activation='linear')(Concatenate()([shared, t0_shared]))\n",
    "    model = Model(inputs=[x, t0, t1], outputs=[t0_output, t1_output])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Subgroup identification using Siamese Neural Network\n",
    "def subgroup_identification(data, treatment_col, outcome_col, features, hidden_dim, dropout_prob, epochs, k):\n",
    "    t0_data = data[data[treatment_col] == 0].copy()\n",
    "    t1_data = data[data[treatment_col] == 1].copy()\n",
    "    mx_dim = min(t0_data.shape[0], t1_data.shape[0])\n",
    "    t0_data = t0_data.head(mx_dim)\n",
    "    t1_data = t1_data.head(mx_dim)\n",
    "    siamese_nn = create_siamese_nn(len(features), hidden_dim, dropout_prob)\n",
    "    siamese_nn.fit([t0_data[features], t0_data[treatment_col], t1_data[treatment_col]], [t0_data[outcome_col], t1_data[outcome_col]], epochs=epochs, verbose=0)\n",
    "    t0_effects, t1_effects = siamese_nn.predict([data[features], data[treatment_col], data[treatment_col]])\n",
    "    abs_effects = abs(t1_effects - t0_effects)\n",
    "    subgroup = data[abs_effects > k]\n",
    "    return subgroup\n",
    "\n",
    "# Identify the subgroup using the Siamese Neural Network\n",
    "hidden_dim = 200\n",
    "dropout_prob = 0.5\n",
    "epochs = 100\n",
    "k = 0.72\n",
    "subgroup = subgroup_identification(df_balanced_final, 'determinant_13', 'opioid_pr_ab', features, hidden_dim, dropout_prob, epochs, k)\n",
    "\n",
    "# Remove duplicate rows from the subgroup\n",
    "subgroup = subgroup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Subsample 10,000 data points from the identified subgroup\n",
    "subsampled_df2 = subgroup.sample(n=10000, random_state=42) if len(subgroup) > 10000 else subgroup\n",
    "\n",
    "# Handle NaN or infinite values before converting to integers\n",
    "subsampled_df2[cols_to_balance] = subsampled_df2[cols_to_balance].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Check the class counts in the final subsampled dataframe\n",
    "print(\"Class counts in the final subsampled dataframe:\")\n",
    "for col in cols_to_balance:\n",
    "    if col in subsampled_df2.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(subsampled_df2[col].value_counts())\n",
    "\n",
    "print(f\"Subgroup shape: {subgroup.shape}\")\n",
    "print(f\"Subsampled dataframe shape: {subsampled_df2.shape}\")\n",
    "\n",
    "# Print the columns in the subgroup\n",
    "print(\"Columns in the subgroup:\")\n",
    "print(subgroup.columns)\n",
    "\n",
    "# Print the columns in the subsampled dataframe\n",
    "print(\"Columns in the subsampled dataframe:\")\n",
    "print(subsampled_df2.columns)\n",
    "\n",
    "# Step 3: Causal Inference Calculation\n",
    "# Ensure that the outcome column is numeric in subsampled_df2\n",
    "subsampled_df2['opioid_pr_ab'] = pd.to_numeric(subsampled_df2['opioid_pr_ab'], errors='coerce').astype(int)\n",
    "\n",
    "# Define the outcome and confounders\n",
    "outcome = 'opioid_pr_ab'\n",
    "#embedding_columns = subsampled_df2.columns[13:].tolist()  \n",
    "#confounders = embedding_columns\n",
    "confounders  = [str(i) for i in range(768)]   # 768-dim BioClinicalBERT embeddings\n",
    "\n",
    "\n",
    "# Function to calculate propensity scores\n",
    "def calculate_propensity_scores(df, treatment, confounders):\n",
    "    X = df[confounders].values\n",
    "    y = df[treatment]\n",
    "    if len(y.unique()) == 2:\n",
    "        model = LogisticRegression(max_iter=5000)\n",
    "        model.fit(X, y)\n",
    "        propensity_scores = model.predict_proba(X)[:, 1]\n",
    "        return propensity_scores\n",
    "    else:\n",
    "        raise ValueError(f\"The target variable '{treatment}' is not binary.\")\n",
    "\n",
    "# Function to run propensity score matching and calculate ATE\n",
    "def run_ps(sampled_df, confounders, treatment, outcome):\n",
    "    X_data = sampled_df[confounders].values\n",
    "    y_data = sampled_df[outcome]\n",
    "    ps = LogisticRegression(max_iter=5000, C=1e6, n_jobs=-1).fit(X_data, sampled_df[treatment]).predict_proba(X_data)[:, 1]\n",
    "    weight = (sampled_df[treatment] - ps) / (ps * (1 - ps))\n",
    "    return np.mean(weight * sampled_df[outcome])\n",
    "\n",
    "# Function to calculate CATE\n",
    "def calculate_cate(sampled_df, treatment, outcome):\n",
    "    treated_df = sampled_df[sampled_df[treatment] == 1]\n",
    "    untreated_df = sampled_df[sampled_df[treatment] == 0]\n",
    "    cate = treated_df[outcome].mean() - untreated_df[outcome].mean()\n",
    "    return cate\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"lbfgs failed to converge\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas requires version\")\n",
    "\n",
    "# Drop the determinant_pr_ab column if it exists\n",
    "if 'determinant_pr_ab' in subsampled_df2.columns:\n",
    "    subsampled_df2 = subsampled_df2.drop(columns=['determinant_pr_ab'])\n",
    "\n",
    "# Define the treatment column (only determinant 13)\n",
    "treatment_columns = ['determinant_13']\n",
    "\n",
    "# Loop through each determinant\n",
    "for treatment in treatment_columns:\n",
    "    print(f\"Calculating for {treatment}\")\n",
    "    subsampled_df2[treatment] = pd.to_numeric(subsampled_df2[treatment], errors='coerce')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = subsampled_df2[treatment].value_counts()\n",
    "    print(f\"Class distribution for {treatment}:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    try:\n",
    "        if len(class_counts) == 2:\n",
    "            subsampled_df2['propensity_score'] = calculate_propensity_scores(subsampled_df2, treatment, confounders)\n",
    "            treated_df = subsampled_df2[subsampled_df2[treatment] == 1]\n",
    "            untreated_df = subsampled_df2[subsampled_df2[treatment] == 0]\n",
    "            weight_t = 1 / treated_df[\"propensity_score\"]\n",
    "            weight_nt = 1 / (1 - untreated_df[\"propensity_score\"])\n",
    "            y1 = sum(treated_df[outcome] * weight_t) / len(weight_t)\n",
    "            y0 = sum(untreated_df[outcome] * weight_nt) / len(weight_nt)\n",
    "            bootstrap_sample = 1000\n",
    "\n",
    "            # Define a function to run within the parallel loop\n",
    "            def run_parallel(sample_idx):\n",
    "                sample = subsampled_df2.sample(frac=1, replace=True, random_state=sample_idx).reset_index(drop=True)\n",
    "                return run_ps(sample, confounders, treatment, outcome)\n",
    "\n",
    "            # Run bootstrap samples in parallel\n",
    "            ates = Parallel(n_jobs=-1)(\n",
    "                delayed(run_parallel)(sample_idx) for sample_idx in range(bootstrap_sample)\n",
    "            )\n",
    "            ates = np.array(ates)\n",
    "            ci_lower = np.percentile(ates, 2.5)\n",
    "            ci_upper = np.percentile(ates, 97.5)\n",
    "            ATE = np.mean(ates)\n",
    "            CATE = calculate_cate(subsampled_df2, treatment, outcome)\n",
    "            model = sm.OLS(subsampled_df2[outcome], sm.add_constant(subsampled_df2[[treatment, 'propensity_score']].astype(float)))\n",
    "            result = model.fit()\n",
    "            p_value = result.pvalues[treatment]\n",
    "            original_sample_size = len(subsampled_df2)\n",
    "            treated_sample_size = len(treated_df)\n",
    "            untreated_sample_size = len(untreated_df)\n",
    "            results.append({\n",
    "                'Determinant': treatment,\n",
    "                'Original Sample Size': original_sample_size,\n",
    "                'Treated Sample Size': treated_sample_size,\n",
    "                'Untreated Sample Size': untreated_sample_size,\n",
    "                'Y1': y1,\n",
    "                'Y0': y0,\n",
    "                'ATE': ATE,\n",
    "                'CATE': CATE,\n",
    "                'p-value': p_value,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping {treatment}: This solver needs samples of at least 2 classes in the data, but the data contains only one class: {class_counts.index[0]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {treatment}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "#results_df.to_csv('causal_inference_results_determinant_13.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
